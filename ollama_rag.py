"""
Ollamaì™€ ChromaDBë¥¼ í™œìš©í•œ ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ
"""

import os
import argparse
from typing import List
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.chains.combine_documents import create_stuff_documents_chain
from prompts.rag_prompt import DEFAULT_RAG_PROMPT, EXPERT_RAG_PROMPT, CONCISE_RAG_PROMPT, create_custom_prompt

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://localhost:11434")

# ë¬¸ì„œ ë¡œë“œ ë° ë¶„í•  í•¨ìˆ˜
def load_and_split_documents(directory_path="./data") -> List:
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    """
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(directory_path):
        os.makedirs(directory_path)
        print(f"'{directory_path}' ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë””ë ‰í† ë¦¬ì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì¶”ê°€í•˜ì„¸ìš”.")
        return []
    
    # ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
    loader = DirectoryLoader(directory_path, glob="**/*.txt", loader_cls=TextLoader)
    documents = loader.load()
    
    if not documents:
        print(f"'{directory_path}' ë””ë ‰í† ë¦¬ì— í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    
    chunks = text_splitter.split_documents(documents)
    print(f"{len(documents)} ë¬¸ì„œë¥¼ {len(chunks)} ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    
    return chunks

# ë²¡í„° ì €ì¥ì†Œ ìƒì„± í•¨ìˆ˜
def create_vector_store(chunks, persist_directory="./chroma_db", model_name="llama3:8b"):
    """
    ë¬¸ì„œ ì²­í¬ë¡œë¶€í„° ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” - Ollama ë˜ëŠ” ë¡œì»¬ HuggingFace ëª¨ë¸ ì¤‘ ì„ íƒ
    try:
        print(f"Ollama ì„ë² ë”© ëª¨ë¸({model_name})ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        embeddings = OllamaEmbeddings(
            model=model_name,
            base_url=OLLAMA_HOST
        )
    except Exception as e:
        print(f"Ollama ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        print("ëŒ€ì²´ HuggingFace ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
        # ëŒ€ì²´ ì„ë² ë”© ëª¨ë¸ë¡œ HuggingFace ì‚¬ìš©
        embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )
    
    # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    
    # persist() ë©”ì„œë“œ í˜¸ì¶œ ì œê±° - ìµœì‹  ë²„ì „ì˜ Chromaì—ì„œëŠ” í•„ìš” ì—†ìŒ
    # ë˜ëŠ” ì¡°ê±´ë¶€ë¡œ ì²˜ë¦¬
    try:
        # ì´ì „ ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•´ ì‹œë„
        if hasattr(vector_store, 'persist'):
            vector_store.persist()
    except Exception as e:
        print(f"ì°¸ê³ : ë²¡í„° ì €ì¥ì†Œ persist í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œë¨): {str(e)}")
    
    print(f"ë²¡í„° ì €ì¥ì†Œê°€ '{persist_directory}'ì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return vector_store

# ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ í•¨ìˆ˜
def load_vector_store(persist_directory="./chroma_db", model_name="llama3:8b"):
    """
    ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    if not os.path.exists(persist_directory):
        print(f"'{persist_directory}'ì— ë²¡í„° ì €ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” - Ollama ë˜ëŠ” ë¡œì»¬ HuggingFace ëª¨ë¸ ì¤‘ ì„ íƒ
    try:
        print(f"Ollama ì„ë² ë”© ëª¨ë¸({model_name})ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        embeddings = OllamaEmbeddings(
            model=model_name,
            base_url=OLLAMA_HOST
        )
    except Exception as e:
        print(f"Ollama ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        print("ëŒ€ì²´ HuggingFace ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
        # ëŒ€ì²´ ì„ë² ë”© ëª¨ë¸ë¡œ HuggingFace ì‚¬ìš©
        embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )
    
    # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
    vector_store = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    
    print(f"ë²¡í„° ì €ì¥ì†Œë¥¼ '{persist_directory}'ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return vector_store

# RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜
def create_rag_chain(retriever, prompt_type="default", custom_instruction=None, model_name="llama3:8b"):
    """
    ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        retriever: ë¬¸ì„œ ê²€ìƒ‰ê¸°
        prompt_type (str): í”„ë¡¬í”„íŠ¸ ìœ í˜• ('default', 'expert', 'concise', 'custom')
        custom_instruction (str, optional): ì‚¬ìš©ì ì •ì˜ ì§€ì‹œì‚¬í•­
        model_name (str): ì‚¬ìš©í•  Ollama ëª¨ë¸ ì´ë¦„
    """
    # LLM ì´ˆê¸°í™”
    try:
        print(f"Ollama LLMì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤({model_name})...")
        llm = ChatOllama(
            model=model_name,
            base_url=OLLAMA_HOST,
            temperature=0.1,  # ì˜¨ë„ ë‚®ì¶¤
            system="ë‹¹ì‹ ì€ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ëª¨ë“  ì§ˆë¬¸ì— ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ì–´ë¡œ ë‹µë³€í•˜ëŠ” ê²ƒì€ ì‹¬ê°í•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì˜ì–´ ëŒ€ì‹  í•œêµ­ì–´ ëŒ€ì²´ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ê³ , ì „ì²´ ë‹µë³€ì„ 100% í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. ì´ê²ƒì€ ì ˆëŒ€ì ì¸ ê·œì¹™ì…ë‹ˆë‹¤."
        )
    except Exception as e:
        print(f"Ollama LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        raise Exception("Ollama LLMì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
    if prompt_type == "expert":
        prompt = EXPERT_RAG_PROMPT
        print("ì „ë¬¸ê°€ ëª¨ë“œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    elif prompt_type == "concise":
        prompt = CONCISE_RAG_PROMPT
        print("ê°„ê²°í•œ ìš”ì•½ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    elif prompt_type == "custom" and custom_instruction:
        prompt = create_custom_prompt(custom_instruction)
        print("ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        prompt = DEFAULT_RAG_PROMPT
        print("ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # ë¬¸ì„œ ì²´ì¸ ìƒì„±
    document_chain = create_stuff_documents_chain(llm, prompt)
    
    # ì²´ì¸ êµ¬ì„±
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | document_chain
    )
    
    return rag_chain

# ë©”ì¸ í•¨ìˆ˜
def main():
    # ëª…ë ¹ì¤„ ì¸ì ì²˜ë¦¬
    parser = argparse.ArgumentParser(description="Ollama RAG ì‹œìŠ¤í…œ")
    parser.add_argument("--prompt", type=str, default="default", 
                       choices=["default", "expert", "concise", "custom"],
                       help="ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ ìœ í˜• (default, expert, concise, custom)")
    parser.add_argument("--custom", type=str, 
                       help="ì‚¬ìš©ì ì •ì˜ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ (--prompt customê³¼ í•¨ê»˜ ì‚¬ìš©)")
    parser.add_argument("--show-sources", action="store_true",
                       help="ê²€ìƒ‰ëœ ë¬¸ì„œ ì²­í¬ë¥¼ í‘œì‹œ")
    parser.add_argument("--model", type=str, default="llama3:8b",
                       help="ì‚¬ìš©í•  Ollama ëª¨ë¸ (ì˜ˆ: llama3:8b, llama3:70b, mistral, gemma:2b)")
    args = parser.parse_args()
    
    # ëª¨ë¸ ì´ë¦„ ì„¤ì •
    model_name = args.model
    print(f"ì‚¬ìš©í•  ëª¨ë¸: {model_name}")
    
    # Ollama ì„œë²„ ì—°ê²° í™•ì¸
    print(f"Ollama ì„œë²„ì— ì—°ê²° ì¤‘... ({OLLAMA_HOST})")
    
    # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±)
    chroma_dir = "./chroma_db"
    vector_store = load_vector_store(chroma_dir, model_name)
    
    if vector_store is None:
        # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
        chunks = load_and_split_documents()
        if chunks:
            vector_store = create_vector_store(chunks, chroma_dir, model_name)
        else:
            print("ë¬¸ì„œê°€ ì—†ì–´ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    
    # ê²€ìƒ‰ê¸° ìƒì„± (ìµœëŒ€ ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜)
    retriever = vector_store.as_retriever(
        search_kwargs={
            "k": 3,
            # "fetch_k": 5,  # ë” ë§ì€ ë¬¸ì„œ ê²€í†  - ì´ ì˜µì…˜ì€ í˜„ì¬ ChromaDB ë²„ì „ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ
            # "score_threshold": None  # ì ìˆ˜ ì„ê³„ê°’ ì—†ìŒ - ì´ ì˜µì…˜ë„ í˜„ì¬ ChromaDB ë²„ì „ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ
        }
    )
    
    # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ì™€ ì ìˆ˜ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
    last_retrieved_docs = []
    last_retrieved_scores = []
    
    # RAG ì²´ì¸ ìƒì„±
    try:
        rag_chain = create_rag_chain(retriever, args.prompt, args.custom, model_name)
    except Exception as e:
        print(f"RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return
    
    # í”„ë¡¬í”„íŠ¸ ìœ í˜• ì •ë³´ í‘œì‹œ
    print(f"\ní”„ë¡¬í”„íŠ¸ ìœ í˜•: {args.prompt}")
    if args.prompt == "custom" and args.custom:
        print(f"ì‚¬ìš©ì ì •ì˜ ì§€ì‹œì‚¬í•­: {args.custom}")
    
    # ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
    print("\n=== Ollama RAG ì‹œìŠ¤í…œì— ì§ˆë¬¸í•˜ê¸° (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥) ===")
    print("íŠ¹ë³„ ëª…ë ¹ì–´: 'sources' - ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ì‚¬ìš©ëœ ë¬¸ì„œ ì²­í¬ì™€ ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ")
    
    while True:
        query = input("\nì§ˆë¬¸: ")
        
        # íŠ¹ë³„ ëª…ë ¹ì–´ ì²˜ë¦¬
        if query.lower() == 'exit':
            break
        elif query.lower() in ['sources', 'ì†ŒìŠ¤', 'ì¶œì²˜']:
            if last_retrieved_docs:
                print("\n--- ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ì‚¬ìš©ëœ ë¬¸ì„œ ì²­í¬ ---")
                for i, (doc, score) in enumerate(zip(last_retrieved_docs, last_retrieved_scores), 1):
                    # ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¥¸ ê´€ë ¨ì„± í‘œì‹œ
                    relevance = "ë†’ìŒ ğŸŸ¢" if score > 0.7 else "ì¤‘ê°„ ğŸŸ¡" if score > 0.5 else "ë‚®ìŒ ğŸ”´"
                    
                    print(f"\n[ì²­í¬ {i}] ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                    print(f"ìœ ì‚¬ë„: {score:.4f} (ê´€ë ¨ì„±: {relevance})")
                    content_preview = doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
                    print(f"ë‚´ìš©: {content_preview}")
            else:
                print("\nì•„ì§ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        # ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        try:
            # ë¬¸ì„œ ê²€ìƒ‰ ë° ìœ ì‚¬ë„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
            # ë‹¨ìˆœí™”ëœ ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰ (í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
            docs_with_scores = vector_store.similarity_search_with_score(query, k=3)
            
            # ê²€ìƒ‰ ê²°ê³¼ì™€ ì ìˆ˜ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
            if docs_with_scores:
                docs, scores = zip(*docs_with_scores)
                last_retrieved_docs = docs
                last_retrieved_scores = scores
            else:
                docs = []
                scores = []
                last_retrieved_docs = []
                last_retrieved_scores = []
            
            # ì†ŒìŠ¤ í‘œì‹œ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš° ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ
            if args.show_sources:
                print("\n--- ê²€ìƒ‰ëœ ë¬¸ì„œ ì²­í¬ ---")
                if docs_with_scores:
                    for i, (doc, score) in enumerate(docs_with_scores, 1):
                        source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                        # ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¥¸ ê´€ë ¨ì„± í‘œì‹œ
                        relevance = "ë†’ìŒ ğŸŸ¢" if score > 0.7 else "ì¤‘ê°„ ğŸŸ¡" if score > 0.5 else "ë‚®ìŒ ğŸ”´"
                        
                        print(f"\n[ì²­í¬ {i}] ì¶œì²˜: {source}")
                        print(f"ìœ ì‚¬ë„: {score:.4f} (ê´€ë ¨ì„±: {relevance})")
                        content_preview = doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
                        print(f"ë‚´ìš©: {content_preview}")
                else:
                    print("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                print("\n--- ì‘ë‹µ ìƒì„± ì¤‘... ---")
            
            # ìƒì„±ì— í•„ìš”í•œ ë¬¸ì„œ ëª©ë¡
            if docs:
                # ì‘ë‹µ ìƒì„± (ê¸°ì¡´ ì²´ì¸ ëŒ€ì‹  ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ì „ë‹¬)
                response = rag_chain.invoke(query)
                
                # ìœ ì‚¬ë„ ì ìˆ˜ ì •ë³´ ì¶”ê°€ (ëª¨ë“  ì ìˆ˜ê°€ ë‚®ì„ ê²½ìš°)
                if scores and max(scores) < 0.5:
                    print("\në‹µë³€:", response)
                    print("\nâš ï¸ ì°¸ê³ : ëª¨ë“  ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ìœ ì‚¬ë„ ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤ (ìµœëŒ€ {:.2f}). ë‹µë³€ì´ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.".format(max(scores)))
                else:
                    print("\në‹µë³€:", response)
            else:
                print("\nì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

if __name__ == "__main__":
    main() 