"""
Ollamaì™€ ChromaDBë¥¼ í™œìš©í•œ ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ
"""

import os
import argparse
import wandb
from typing import List
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.chains.combine_documents import create_stuff_documents_chain
from prompts.rag_prompt import DEFAULT_RAG_PROMPT, EXPERT_RAG_PROMPT, CONCISE_RAG_PROMPT, create_custom_prompt
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from dotenv import load_dotenv
from datetime import datetime
from langchain_core.documents import Document

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
load_dotenv()

# Ollama ì„œë²„ ì„¤ì •
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")

# wandb ì´ˆê¸°í™” í•¨ìˆ˜
def init_wandb(project_name="ollama-rag", prompt_type="default", model_name="llama3:8b"):
    """
    Weights & Biases ì´ˆê¸°í™”
    
    Args:
        project_name (str): wandb í”„ë¡œì íŠ¸ ì´ë¦„
        prompt_type (str): í”„ë¡¬í”„íŠ¸ ìœ í˜•
        model_name (str): ëª¨ë¸ ì´ë¦„
    """
    try:
        # wandb ë²„ì „ í™•ì¸
        print(f"ğŸ” wandb ë²„ì „: {wandb.__version__}")
        
        # wandb API í‚¤ê°€ í™˜ê²½ ë³€ìˆ˜ì— ìˆëŠ”ì§€ í™•ì¸
        api_key = os.getenv("WANDB_API_KEY")
        if not api_key:
            print("âš ï¸ WANDB_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. wandb ë¡œê¹…ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            
            # .env íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            env_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.env')
            if os.path.exists(env_path):
                print(f"âœ“ .env íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {env_path}")
                with open(env_path, 'r') as f:
                    env_content = f.read()
                    if "WANDB_API_KEY" in env_content:
                        print("âœ“ .env íŒŒì¼ì— WANDB_API_KEYê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        print("âœ— .env íŒŒì¼ì— WANDB_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"âœ— .env íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {env_path}")
            
            return None
        else:
            # API í‚¤ ì²« 4ìë¦¬ë§Œ ì¶œë ¥ (ë³´ì•ˆìƒ ì´ìœ ë¡œ)
            api_key_preview = api_key[:4] + "..." if len(api_key) > 4 else "ìœ íš¨í•˜ì§€ ì•Šì€ í˜•ì‹"
            print(f"âœ“ WANDB_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (ì‹œì‘: {api_key_preview})")
        
        # ì‹¤í—˜ ì´ë¦„ ìƒì„± (ëª¨ë¸ + í”„ë¡¬í”„íŠ¸ íƒ€ì…)
        experiment_name = f"{model_name}_{prompt_type}"
        print(f"ğŸ”„ wandb ì‹¤í—˜ ì´ë¦„: {experiment_name}")
        
        # wandb ì´ˆê¸°í™”
        print("ğŸ”„ wandb ì´ˆê¸°í™” ì¤‘...")
        run = wandb.init(
            project=project_name,
            name=experiment_name,
            config={
                "prompt_type": prompt_type,
                "model_name": model_name,
                "chunk_size": 1000,
                "chunk_overlap": 200,
                "retriever_k": 1,
                "timestamp": datetime.now().strftime('%Y%m%d_%H%M%S')
            },
            # ê°™ì€ ì´ë¦„ì˜ ì‹¤í—˜ì´ ìˆìœ¼ë©´ ì¬ê°œ(resume)
            resume="allow"
        )
        
        print(f"âœ“ wandb ì´ˆê¸°í™” ì„±ê³µ: {run.id}")
        return run
    except Exception as e:
        print(f"âœ— wandb ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return None

# ë¬¸ì„œ ë¡œë“œ ë° ë¶„í•  í•¨ìˆ˜
def load_documents(directory: str) -> List:
    """ë¬¸ì„œ ë¡œë“œ"""
    documents = []
    
    # ë””ë²„ê¹… ì •ë³´
    print(f"ğŸ“‚ ë¬¸ì„œë¥¼ ë¡œë“œí•  ë””ë ‰í† ë¦¬: {os.path.abspath(directory)}")
    
    try:
        files = os.listdir(directory)
        print(f"ğŸ“„ ë°œê²¬ëœ íŒŒì¼: {files}")
        
        for filename in files:
            file_path = os.path.join(directory, filename)
            print(f"   í™•ì¸ ì¤‘: {filename} (is_file: {os.path.isfile(file_path)}, endswith .txt: {filename.endswith('.txt')})")
            
            if os.path.isfile(file_path) and filename.endswith(".txt"):
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        text = f.read()
                        print(f"   âœ“ íŒŒì¼ ì½ê¸° ì„±ê³µ: {filename} (ê¸¸ì´: {len(text)} ì)")
                        # Document ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
                        documents.append(Document(page_content=text, metadata={"source": filename}))
                except Exception as e:
                    print(f"   âœ— íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {filename}, ì˜¤ë¥˜: {str(e)}")
    except Exception as e:
        print(f"ë””ë ‰í† ë¦¬ íƒìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    print(f"ğŸ”¢ ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    return documents

def split_documents(documents: List, chunk_size: int = 1000, chunk_overlap: int = 200) -> List:
    """ë¬¸ì„œ ë¶„í• """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    return text_splitter.split_documents(documents)

# ë²¡í„° ì €ì¥ì†Œ ìƒì„± í•¨ìˆ˜
def create_vector_store(documents: List, persist_directory: str = "./chroma_db") -> Chroma:
    """ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
    embeddings = OllamaEmbeddings(
        model="llama3:8b",
        base_url=OLLAMA_HOST
    )
    
    vector_store = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    
    try:
        vector_store.persist()
    except AttributeError:
        print("Warning: persist() ë©”ì„œë“œê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë²¡í„° ì €ì¥ì†ŒëŠ” ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥ë©ë‹ˆë‹¤.")
    
    return vector_store

# ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ í•¨ìˆ˜
def load_vector_store(persist_directory="./chroma_db", model_name="llama3:8b"):
    """
    ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    if not os.path.exists(persist_directory):
        print(f"'{persist_directory}'ì— ë²¡í„° ì €ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” - Ollama ë˜ëŠ” ë¡œì»¬ HuggingFace ëª¨ë¸ ì¤‘ ì„ íƒ
    try:
        print(f"Ollama ì„ë² ë”© ëª¨ë¸({model_name})ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        embeddings = OllamaEmbeddings(
            model=model_name,
            base_url=OLLAMA_HOST
        )
    except Exception as e:
        print(f"Ollama ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        print("ëŒ€ì²´ HuggingFace ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
        # ëŒ€ì²´ ì„ë² ë”© ëª¨ë¸ë¡œ HuggingFace ì‚¬ìš©
        embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )
    
    # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
    vector_store = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    
    print(f"ë²¡í„° ì €ì¥ì†Œë¥¼ '{persist_directory}'ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return vector_store

# RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜
def create_rag_chain(retriever, prompt_type="default", custom_instruction=None, model_name='exaone:latest'):
    """
    ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        retriever: ë¬¸ì„œ ê²€ìƒ‰ê¸°
        prompt_type (str): í”„ë¡¬í”„íŠ¸ ìœ í˜• ('default', 'expert', 'concise', 'custom')
        custom_instruction (str, optional): ì‚¬ìš©ì ì •ì˜ ì§€ì‹œì‚¬í•­
        model_name (str): ì‚¬ìš©í•  Ollama ëª¨ë¸ ì´ë¦„
    """
    # LLM ì´ˆê¸°í™”
    try:
        print(f"Ollama LLMì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤({model_name})...")
        llm = ChatOllama(
            model=model_name,
            base_url=OLLAMA_HOST,
            temperature=0.1,  # ì˜¨ë„ ë‚®ì¶¤
            system="ë‹¹ì‹ ì€ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ëª¨ë“  ì§ˆë¬¸ì— ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ì–´ë¡œ ë‹µë³€í•˜ëŠ” ê²ƒì€ ì‹¬ê°í•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì˜ì–´ ëŒ€ì‹  í•œêµ­ì–´ ëŒ€ì²´ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ê³ , ì „ì²´ ë‹µë³€ì„ 100% í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. ì´ê²ƒì€ ì ˆëŒ€ì ì¸ ê·œì¹™ì…ë‹ˆë‹¤."
        )
    except Exception as e:
        print(f"Ollama LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        raise Exception("Ollama LLMì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
    if prompt_type == "expert":
        prompt = EXPERT_RAG_PROMPT
        print("ì „ë¬¸ê°€ ëª¨ë“œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    elif prompt_type == "concise":
        prompt = CONCISE_RAG_PROMPT
        print("ê°„ê²°í•œ ìš”ì•½ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    elif prompt_type == "custom" and custom_instruction:
        prompt = create_custom_prompt(custom_instruction)
        print("ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        prompt = DEFAULT_RAG_PROMPT
        print("ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # ë¬¸ì„œ ì²´ì¸ ìƒì„±
    document_chain = create_stuff_documents_chain(llm, prompt)
    
    # ì²´ì¸ êµ¬ì„±
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | document_chain
    )
    
    return rag_chain

# ë©”ì¸ í•¨ìˆ˜
def main():
    parser = argparse.ArgumentParser(description="Ollama RAG ì‹œìŠ¤í…œ")
    parser.add_argument("--model", default="exaone3.5:latest", help="ì‚¬ìš©í•  Ollama ëª¨ë¸")
    parser.add_argument("--prompt", default="default", choices=["default", "expert", "concise", "custom"],
                      help="í”„ë¡¬í”„íŠ¸ ìœ í˜• ì„ íƒ")
    parser.add_argument("--custom", help="ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ ì§€ì‹œì‚¬í•­")
    args = parser.parse_args()

    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ì ˆëŒ€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì • (ì ˆëŒ€ ê²½ë¡œ)
    DATA_DIR = os.path.join(SCRIPT_DIR, "data")
    CHROMA_DIR = os.path.join(SCRIPT_DIR, "chroma_db")
    
    # wandb ì´ˆê¸°í™”
    wandb_run = init_wandb(prompt_type=args.prompt, model_name=args.model)

    # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)
        print(f"'{DATA_DIR}' ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        return

    # ë¬¸ì„œ ë¡œë“œ
    try:
        documents = load_documents(DATA_DIR)
        if not documents:
            print(f"'{DATA_DIR}' ë””ë ‰í† ë¦¬ì— í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
    except Exception as e:
        print(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return

    # ë¬¸ì„œ ë¶„í• 
    try:
        chunks = split_documents(documents)
        print(f"ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ë¬¸ì„œ ë¶„í•  ì‹¤íŒ¨: {str(e)}")
        return

    # ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë˜ëŠ” ë¡œë“œ
    try:
        if os.path.exists(CHROMA_DIR):
            print(f"ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤({CHROMA_DIR})...")
            vector_store = Chroma(
                persist_directory=CHROMA_DIR,
                embedding_function=OllamaEmbeddings(
                    model=args.model,
                    base_url=OLLAMA_HOST
                )
            )
        else:
            print(f"ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤({CHROMA_DIR})...")
            vector_store = create_vector_store(chunks, persist_directory=CHROMA_DIR)
    except Exception as e:
        print(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„±/ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return
    
    # ê²€ìƒ‰ê¸° ìƒì„± (ìµœëŒ€ ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜)
    retriever = vector_store.as_retriever(
        search_kwargs={
            "k": 1,
        }
    )
    
    # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ì™€ ì ìˆ˜ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
    last_retrieved_docs = []
    last_retrieved_scores = []
    
    # RAG ì²´ì¸ ìƒì„±
    try:
        rag_chain = create_rag_chain(retriever, args.prompt, args.custom, args.model)
    except Exception as e:
        print(f"RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return
    
    # í”„ë¡¬í”„íŠ¸ ìœ í˜• ì •ë³´ í‘œì‹œ
    print(f"\ní”„ë¡¬í”„íŠ¸ ìœ í˜•: {args.prompt}")
    if args.prompt == "custom" and args.custom:
        print(f"ì‚¬ìš©ì ì •ì˜ ì§€ì‹œì‚¬í•­: {args.custom}")
    
    # ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
    print("\n=== Ollama RAG ì‹œìŠ¤í…œì— ì§ˆë¬¸í•˜ê¸° (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥) ===")
    print("íŠ¹ë³„ ëª…ë ¹ì–´: 'sources' - ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ì‚¬ìš©ëœ ë¬¸ì„œ ì²­í¬ì™€ ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ")
    
    # ì§ˆë¬¸ ì¹´ìš´í„° ë° ê¸°ë¡ ë³€ìˆ˜ ì´ˆê¸°í™”
    question_count = 0
    response_times = []
    last_query = "ì—†ìŒ"
    last_response = "ì—†ìŒ"
    
    while True:
        query = input("\nì§ˆë¬¸: ")
        
        # íŠ¹ë³„ ëª…ë ¹ì–´ ì²˜ë¦¬
        if query.lower() == 'exit':
            break

        # ì§ˆë¬¸ ì¹´ìš´í„° ì¦ê°€
        question_count += 1
        
        # ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        try:
            # ë¬¸ì„œ ê²€ìƒ‰ ë° ìœ ì‚¬ë„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
            docs_with_scores = vector_store.similarity_search_with_score(query, k=1)
            
            # ê²€ìƒ‰ ê²°ê³¼ì™€ ì ìˆ˜ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
            docs = [doc for doc, _ in docs_with_scores]
            scores = [score for _, score in docs_with_scores]
            
            # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ì™€ ì ìˆ˜ ì—…ë°ì´íŠ¸
            last_retrieved_docs = docs
            last_retrieved_scores = scores
            
            if docs:
                # ì‘ë‹µ ìƒì„± ì‹œê°„ ì¸¡ì •
                start_time = datetime.now()
                response = rag_chain.invoke(query)
                end_time = datetime.now()
                response_time = (end_time - start_time).total_seconds()
                
                # ë§ˆì§€ë§‰ ì§ˆë¬¸ê³¼ ì‘ë‹µ ì—…ë°ì´íŠ¸
                last_query = query
                last_response = response
                response_times.append(response_time)
                
                # wandb ë¡œê¹…
                if wandb_run:
                    try:
                        print(f"ğŸ”„ wandbì— ë¡œê¹… ì‹œë„ ì¤‘... (ì§ˆë¬¸ #{question_count})")
                        # ì§ˆë¬¸ ê·¸ë£¹ ì„¤ì • (ëª¨ë¸_í”„ë¡¬í”„íŠ¸ ì¡°í•©ìœ¼ë¡œ ê·¸ë£¹í™”)
                        group_id = f"{args.model}_{args.prompt}"
                        
                        wandb.log({
                            "step": question_count,
                            "group": group_id,
                            "query": query,
                            "response": response,
                            "response_time": response_time,
                            "avg_similarity_score": sum(scores) / len(scores),
                            "max_similarity_score": max(scores),
                            "min_similarity_score": min(scores),
                            "num_retrieved_docs": len(docs)
                        })
                        
                        # ê°œë³„ ì§ˆë¬¸-ì‘ë‹µ ìŒì„ í…Œì´ë¸”ë¡œ ì €ì¥
                        qa_table = wandb.Table(columns=["ì§ˆë¬¸", "ì‘ë‹µ", "ì‘ë‹µì‹œê°„(ì´ˆ)", "í‰ê· ìœ ì‚¬ë„"])
                        qa_table.add_data(query, response, response_time, sum(scores) / len(scores))
                        wandb.log({f"QA_pair_{question_count}": qa_table})
                        
                        print(f"âœ“ wandb ë¡œê¹… ì„±ê³µ")
                    except Exception as e:
                        print(f"âœ— wandb ë¡œê¹… ì˜¤ë¥˜: {str(e)}")
                        import traceback
                        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                else:
                    print("! wandb_runì´ Noneì…ë‹ˆë‹¤. ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                
                # ìœ ì‚¬ë„ ì ìˆ˜ ì •ë³´ ì¶”ê°€ (ëª¨ë“  ì ìˆ˜ê°€ ë‚®ì„ ê²½ìš°)
                if all(score < 0.5 for score in scores):
                    print("\nâš ï¸ ì£¼ì˜: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ìœ ì‚¬ë„ê°€ ëª¨ë‘ ë‚®ìŠµë‹ˆë‹¤. ë‹µë³€ì˜ ì •í™•ì„±ì„ ì‹ ì¤‘í•˜ê²Œ ê²€í† í•´ì£¼ì„¸ìš”.")
                
                # ë‹µë³€ ì¶œë ¥
                print(f"\në‹µë³€: {response}")
                print(f"ìœ ì‚¬ë„ ì ìˆ˜: {scores}")
                
                # ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ ë¬¸ì„œì™€ ìœ ì‚¬ë„ ìë™ ì¶œë ¥
                print("\n--- ì°¸ì¡°ëœ ë¬¸ì„œ ì •ë³´ ---")
                for i, (doc, score) in enumerate(zip(docs, scores), 1):
                    relevance = "ë†’ìŒ ğŸŸ¢" if score > 0.7 else "ì¤‘ê°„ ğŸŸ¡" if score > 0.5 else "ë‚®ìŒ ğŸ”´"
                    print(f"[ë¬¸ì„œ {i}] ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                    print(f"ìœ ì‚¬ë„: {score:.4f} (ê´€ë ¨ì„±: {relevance})")
            else:
                print("\nì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            # ì˜¤ë¥˜ ë¡œê¹…
            if wandb_run:
                wandb.log({
                    "error": str(e),
                    "error_traceback": traceback.format_exc()
                })

    # ì‹¤í—˜ ìš”ì•½ ì •ë³´ ê¸°ë¡ ë° ì¢…ë£Œ
    if wandb_run and question_count > 0:
        try:
            print(f"ğŸ”„ wandbì— ì‹¤í—˜ ìš”ì•½ ì •ë³´ ê¸°ë¡ ì¤‘...")
            # ëª¨ë“  ì§ˆë¬¸-ì‘ë‹µ ìŒì„ ìš”ì•½í•œ í…Œì´ë¸” ìƒì„±
            summary_table = wandb.Table(columns=["ì´ ì§ˆë¬¸ ìˆ˜", "í‰ê·  ì‘ë‹µ ì‹œê°„", "ë§ˆì§€ë§‰ ì§ˆë¬¸", "ë§ˆì§€ë§‰ ì‘ë‹µ"])
            
            # ì‹¤í—˜ ì „ì²´ í†µê³„ ì €ì¥
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0
            
            summary_table.add_data(question_count, avg_response_time, last_query, last_response)
            wandb.log({"experiment_summary": summary_table})
            
            print(f"âœ“ wandb ì‹¤í—˜ ìš”ì•½ ì •ë³´ ê¸°ë¡ ì„±ê³µ")
            
            print(f"\nì‹¤í—˜ ìš”ì•½:")
            print(f"ì´ ì§ˆë¬¸ ìˆ˜: {question_count}")
            print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_response_time:.4f}ì´ˆ")
        except Exception as e:
            print(f"âœ— wandb ì‹¤í—˜ ìš”ì•½ ê¸°ë¡ ì˜¤ë¥˜: {str(e)}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    # wandb ì¢…ë£Œ
    if wandb_run:
        try:
            print("ğŸ”„ wandb ì¢…ë£Œ ì¤‘...")
            wandb.finish()
            print("âœ“ wandb ì¢…ë£Œ ì„±ê³µ")
        except Exception as e:
            print(f"âœ— wandb ì¢…ë£Œ ì˜¤ë¥˜: {str(e)}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

if __name__ == "__main__":
    main() 